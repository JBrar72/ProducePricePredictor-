# -*- coding: utf-8 -*-
"""4010_Project_Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1athpAZYEND9OsWf2Tslxi1ALhipq9aQ9

## Predictions Pertaining Produce Prices ##

Models:
-
"""

# Optional install if needed
# !pip install pmdarima==2.0.4

# library imports
import os, warnings, math, random, time
warnings.filterwarnings("ignore") # supressing warnings
os.makedirs('models', exist_ok=True) # for saving data to local runtime

# more imports
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import joblib
from datetime import timedelta

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error

import statsmodels.api as sm
from statsmodels.tsa.statespace.sarimax import SARIMAX

# for deep learning
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, callbacks

### 1 - PARAMETERS - CHANGE AS NEEDED ###

DATA_PATH = '/content/dataset.csv'
COMMODITIES = ["Tomato Big(Nepali)", "Potato White"] # chosen as they're common
# and popular types of vegetables, can change this to anything
INPUT_WINDOW = 10
HORIZON = 10 # we do 10 day prediction (not too short not too long)
TEST_FRACTION = 0.20 # 20% testing split

BASE_EPOCHS = 30 # the default (not too small, but not too high as we really
# didn't want training this to take very long)
BATCH_SIZE = 32 # default batch size

# Hyperparameter Optimization stuff (HPO)
HPO_TRIALS = 8
HPO_EPOCHS = 20   # smaller because this takes a long time
SEED = 42
SAVE_RESULTS = False   # set to True if you want things saved

np.random.seed(SEED)
tf.random.set_seed(SEED)
random.seed(SEED)

### 2 - HELPER FUNCTIONS ###

# computes the RMSE and MAE for each horizon
def multistep_rmse_mae(y_true, y_pred):
    rmses = np.sqrt(np.mean((y_true - y_pred)**2, axis=0))
    maes  = np.mean(np.abs(y_true - y_pred), axis=0)
    # computes the overall RMSE/MAE over the flattened arrays
    overall_rmse = math.sqrt(mean_squared_error(y_true.flatten(), y_pred.flatten()))
    overall_mae  = mean_absolute_error(y_true.flatten(), y_pred.flatten())
    return overall_rmse, overall_mae, rmses, maes

def create_multistep_windows(series_values, input_len=INPUT_WINDOW, horizon=HORIZON):
  # supervised learning windows from 1D time series
    X, y = [], []
    n = len(series_values)
    for i in range(n - input_len - horizon + 1):
        X.append(series_values[i : i + input_len]) # past window
        y.append(series_values[i + input_len : i + input_len + horizon]) # future window
    return np.array(X), np.array(y)

def inv_scale_multi(pred_scaled, scaler):
  # inverse transform multi-step scaled predictions
    n, h = pred_scaled.shape
    out_cols = []
    for j in range(h):
        col = pred_scaled[:, j].reshape(-1,1)
        out_col = scaler.inverse_transform(col).flatten()
        out_cols.append(out_col)
    return np.vstack(out_cols).T

# plots the series + the predicted first horizon
def plot_single_model_prediction(dates_full, series_full, dates_pred, pred_first_h, model_name, commodity_name):
    plt.figure(figsize=(12,5))
    plt.plot(dates_full, series_full, label="Actual (full)", alpha=0.7)
    plt.plot(dates_pred, pred_first_h, label=f"{model_name} pred t+1", linestyle='--')
    plt.title(f"{commodity_name} - {model_name} First-Horizon Prediction")
    plt.xlabel("Date"); plt.ylabel("Price"); plt.legend(); plt.grid(True); plt.show()

# plots comparison between training and validation losses over each epoch
def plot_training_history(history, model_name, commodity_name):
    if history is None:
        return
    plt.figure(figsize=(10,4))
    plt.plot(history.history.get("loss", []), label="Train Loss")
    if "val_loss" in history.history:
        plt.plot(history.history.get("val_loss", []), label="Val Loss")
    plt.title(f"{commodity_name} - {model_name} Training History")
    plt.xlabel("Epoch"); plt.ylabel("Loss"); plt.legend(); plt.grid(True); plt.show()

def plot_true_vs_pred_first_h(y_true, y_pred, model_name, commodity_name):
    # Scatterplot of true vs predicted values for t+1 horizon
    if y_true is None or y_pred is None or y_true.size==0 or y_pred.size==0:
        return
    yt = y_true[:,0].flatten()
    yp = y_pred[:,0].flatten()
    plt.figure(figsize=(6,6))
    plt.scatter(yt, yp, alpha=0.6)
    mn = min(yt.min(), yp.min()); mx = max(yt.max(), yp.max())
    plt.plot([mn,mx],[mn,mx], color='red', linestyle='--')
    plt.title(f"{commodity_name} - {model_name} True vs Pred (t+1)")
    plt.xlabel("True t+1"); plt.ylabel("Predicted t+1"); plt.grid(True); plt.show()

def plot_true_vs_pred_flatten(y_true, y_pred, model_name, commodity_name, max_points=5000):
    # Flattening multi-horizon into 1D arrays and do scatterplot
    if y_true is None or y_pred is None or y_true.size==0 or y_pred.size==0:
        return
    yt = y_true.flatten()
    yp = y_pred.flatten()
    n = len(yt)
    if n > max_points:
        idx = np.random.choice(n, max_points, replace=False)
        yt = yt[idx]; yp = yp[idx]
    plt.figure(figsize=(6,6))
    plt.scatter(yt, yp, alpha=0.4, s=10)
    mn = min(yt.min(), yp.min()); mx = max(yt.max(), yp.max())
    plt.plot([mn,mx],[mn,mx], color='red', linestyle='--')
    plt.title(f"{commodity_name} - {model_name} True vs Pred (flattened {len(yt)} points)")
    plt.xlabel("True (all horizons)"); plt.ylabel("Predicted (all horizons)"); plt.grid(True); plt.show()

### 3 - MODEL BUILDERS ###

# CNN
def build_cnn_from_params(params, input_len=INPUT_WINDOW, horizon=HORIZON):
    model = keras.Sequential()
    model.add(keras.Input(shape=(input_len,1)))
    model.add(layers.Conv1D(params['filters1'], 3, activation='relu', padding='causal')) # first convolutional layer
    model.add(layers.Conv1D(params['filters2'], 3, activation='relu', padding='causal')) # second conv layer
    model.add(layers.Flatten())
    model.add(layers.Dense(params['dense'], activation='relu'))
    model.add(layers.Dropout(params['dropout'])) # regularization
    model.add(layers.Dense(horizon)) # we are predicting next horizon
    model.compile(optimizer=keras.optimizers.Adam(params['lr']), loss='mse', metrics=['mae'])
    return model

# Recurrent Neural Network
def build_rnn_from_params(params, input_len=INPUT_WINDOW, horizon=HORIZON):
    model = keras.Sequential([
        keras.Input(shape=(input_len,1)), #need 3d input
        layers.SimpleRNN(params['units'], activation='tanh'),#simple reccurent layer
        layers.Dense(params['dense'], activation='relu'), # dense hidden layer
        layers.Dropout(params['dropout']),
        layers.Dense(horizon)
    ])
    model.compile(optimizer=keras.optimizers.Adam(params['lr']), loss='mse', metrics=['mae'])
    return model

# Long Short-Term Memory
def build_lstm_from_params(params, input_len=INPUT_WINDOW, horizon=HORIZON):
    model = keras.Sequential([
        keras.Input(shape=(input_len,1)),
        layers.LSTM(params['units']),
        layers.Dense(params['dense'], activation='relu'),
        layers.Dropout(params['dropout']),
        layers.Dense(horizon)
    ])
    model.compile(optimizer=keras.optimizers.Adam(params['lr']), loss='mse', metrics=['mae'])
    return model

# Gated Recurrent Unit
def build_gru_from_params(params, input_len=INPUT_WINDOW, horizon=HORIZON):
    model = keras.Sequential([
        keras.Input(shape=(input_len,1)),
        layers.GRU(params['units']),
        layers.Dense(params['dense'], activation='relu'),
        layers.Dropout(params['dropout']),
        layers.Dense(horizon)
    ])
    model.compile(optimizer=keras.optimizers.Adam(params['lr']), loss='mse', metrics=['mae'])
    return model

# transformer model
def transformer_block(x, head_count=4, ff_dim=64, dropout_rate=0.1):
  # single encoder block
    dim = int(x.shape[-1])
    # multi-head self-attention
    attn = layers.MultiHeadAttention(num_heads=head_count, key_dim=max(1, dim//head_count))(x, x)
    # skip connection + layer normalization
    x = layers.Add()([x, attn])
    x = layers.LayerNormalization(epsilon=1e-6)(x)
    # feed forward network
    ff = layers.Conv1D(filters=ff_dim, kernel_size=1, activation='relu')(x)
    ff = layers.Conv1D(filters=dim, kernel_size=1)(ff)
    # skip connection + layer normalization
    x = layers.Add()([x, ff])
    x = layers.LayerNormalization(epsilon=1e-6)(x)
    return layers.Dropout(dropout_rate)(x)

# transformer model based on HPO just like all the others
def build_transformer_from_params(params, input_len=INPUT_WINDOW, horizon=HORIZON):
    embed_dim = params['embed_dim']
    head_count = params['heads']
    ff_dim = params['ff_dim']
    n_blocks = params['blocks']
    dropout = params['dropout']
    inputs = keras.Input(shape=(input_len,1))
    x = layers.Dense(embed_dim)(inputs)
    for _ in range(n_blocks):
        x = transformer_block(x, head_count=head_count, ff_dim=ff_dim, dropout_rate=dropout)
    x = layers.Flatten()(x)
    x = layers.Dense(128, activation='relu')(x)
    x = layers.Dropout(dropout)(x)
    outputs = layers.Dense(horizon)(x)
    model = keras.Model(inputs=inputs, outputs=outputs)
    model.compile(optimizer=keras.optimizers.Adam(params['lr']), loss='mse', metrics=['mae'])
    return model

### 4 - Hyperparameter Optimization ###

# because it asked us to do it in our progress report feedback
# randomly samples a set of hyperparameters from a defined search spae
def sample_params(space):
    params = {}
    for k,v in space.items():
        if isinstance(v, list): # if v is a list choose random item
            params[k] = random.choice(v)
        elif isinstance(v, tuple) and len(v)==3 and v[0]=='int_range': # if integer range, format it
            params[k] = random.randrange(v[1], v[2]+1, v[3] if len(v)>3 else 1)
        elif isinstance(v, tuple) and len(v)==3 and v[0]=='log_uniform': # good for learning rates
            low, high = v[1], v[2]
            params[k] = 10**random.uniform(math.log10(low), math.log10(high))
        else: # if gives literal values then just copy it
            params[k] = v
    return params

def run_random_hpo(model_name, build_fn, space, X_train, y_train, X_val, y_val, trials=HPO_TRIALS):
    best = {'val_loss': np.inf, 'params': None}
    for t in range(trials):
        hp = sample_params(space)
        model = build_fn(hp) # builds model with sampled parameters
        es = callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True) # early stopping
        history = model.fit(X_train, y_train, validation_data=(X_val,y_val), epochs=HPO_EPOCHS, batch_size=BATCH_SIZE, callbacks=[es], verbose=0)
        val_loss = min(history.history['val_loss']) # use best validation loss seen
        print(f"HPO {model_name} trial {t+1}/{trials} val_loss={val_loss:.6f} hp={hp}")
        if val_loss < best['val_loss']:
            best['val_loss']=val_loss; best['params']=hp
    print(f"Best HPO for {model_name}: val_loss={best['val_loss']:.6f} params={best['params']}") # summary
    final_model = build_fn(best['params']) # build new model with best params
    return best['params'], final_model

### 5 - ARIMA and SARIMA ###

def run_arima_forecast(series, train_ratio=1 - TEST_FRACTION, order=(1,1,1), horizon=HORIZON):
    s = np.asarray(series).astype(float) # convert to float numpy array
    n = len(s)
    train_end = int(n * train_ratio) # determine end of training portion
    train = s[:train_end]
    test = s[train_end:]
    model = sm.tsa.ARIMA(train, order=order) # fit arima on training set
    res = model.fit()
    preds = []
    history = list(train) # rolling window of data
    for t in range(len(test)):
        model_t = sm.tsa.ARIMA(history, order=order)
        res_t = model_t.fit()
        p = res_t.forecast(steps=1)[0]
        preds.append(p)
        history.append(test[t])
    preds = np.array(preds)
    per_h_preds_aligned = []
    per_h_true_aligned = []
    for h in range(1, HORIZON+1):
        if (len(preds) - (h-1)) <= 0:
            per_h_preds_aligned.append(np.array([])); per_h_true_aligned.append(np.array([]))
        else:
            per_h_preds_aligned.append(preds[(h-1):])
            per_h_true_aligned.append(np.array(test)[:len(preds)-(h-1)])
    eff = max(0, len(test) - (HORIZON - 1))
    if eff > 0:
        Y_true = np.zeros((eff, HORIZON)); Y_pred = np.zeros((eff, HORIZON))
        for i in range(eff):
            for h in range(HORIZON):
                Y_true[i,h] = per_h_true_aligned[h][i]; Y_pred[i,h] = per_h_preds_aligned[h][i]
        overall_rmse, overall_mae, rmses, maes = multistep_rmse_mae(Y_true, Y_pred)
    else:
        Y_true = None; Y_pred = None; overall_rmse = np.nan; overall_mae = np.nan; rmses=[np.nan]*HORIZON
    return {'Y_true': Y_true, 'Y_pred': Y_pred, 'overall_rmse': overall_rmse, 'overall_mae': overall_mae, 'per_horizon_rmse': rmses}

def run_iterative_sarima(series, train_ratio=1 - TEST_FRACTION, order=(1,1,1), seasonal_order=(0,1,1,7), refit_every=10):
    data = np.asarray(series).astype(float)
    n = len(data)
    train_end = int(n * train_ratio)
    train = data[:train_end].tolist()
    test = data[train_end:].tolist()

    model = SARIMAX(train, order=order, seasonal_order=seasonal_order,
                    enforce_stationarity=False, enforce_invertibility=False)
    try:
        res = model.fit(disp=False, method='lbfgs', maxiter=50)
    except Exception:
        res = model.fit(disp=False, method='nm', maxiter=20)

    preds_one_step = []
    history = train.copy()

    for i in range(len(test)):
        try:
            f = res.forecast(steps=1)[0]
        except Exception:
            f = res.predict(start=len(history), end=len(history))[0]
        preds_one_step.append(f)
        history.append(test[i])
        try:
            res = res.append([test[i]], refit=False)
        except Exception:
            model = SARIMAX(history, order=order, seasonal_order=seasonal_order,
                            enforce_stationarity=False, enforce_invertibility=False)
            try:
                res = model.filter(res.params)
            except Exception:
                res = model.fit(disp=False, method='nm', maxiter=10)
        if (i>0) and (i % refit_every == 0):
            model = SARIMAX(history, order=order, seasonal_order=seasonal_order,
                            enforce_stationarity=False, enforce_invertibility=False)
            try:
                res = model.fit(disp=False, method='lbfgs', maxiter=50)
            except Exception:
                res = model.fit(disp=False, method='nm', maxiter=20)

    preds_one_step = np.array(preds_one_step)
    per_h_preds_aligned = []
    per_h_true_aligned = []
    for h in range(1, HORIZON+1):
        if (len(preds_one_step) - (h-1)) <= 0:
            per_h_preds_aligned.append(np.array([])); per_h_true_aligned.append(np.array([]))
        else:
            per_h_preds_aligned.append(preds_one_step[(h-1):])
            per_h_true_aligned.append(np.array(test)[:len(preds_one_step)-(h-1)])
    eff = max(0, len(test) - (HORIZON - 1))
    if eff > 0:
        Y_true = np.zeros((eff, HORIZON)); Y_pred = np.zeros((eff, HORIZON))
        for i in range(eff):
            for h in range(HORIZON):
                Y_true[i,h] = per_h_true_aligned[h][i]; Y_pred[i,h] = per_h_preds_aligned[h][i]
        overall_rmse, overall_mae, rmses, maes = multistep_rmse_mae(Y_true, Y_pred)
    else:
        Y_true = None; Y_pred = None; overall_rmse = np.nan; overall_mae = np.nan; rmses=[np.nan]*HORIZON
    return {'Y_true': Y_true, 'Y_pred': Y_pred, 'overall_rmse': overall_rmse, 'overall_mae': overall_mae, 'per_horizon_rmse': rmses}

### Main Pipeline - Loading data, running models, everything else ###

print("Loading dataset:", DATA_PATH)
df_all = pd.read_csv(DATA_PATH)
df_all['Date'] = pd.to_datetime(df_all['Date'])
print("Available commodities (sample):", df_all['Commodity'].unique()[:20])

final_table = []
hpo_done = False
shared_hyperparams = {}
shared_models = {}

for idx, commodity in enumerate(COMMODITIES):
    print("\n" + "="*80)
    print("Processing commodity:", commodity)
    df_c = df_all[df_all['Commodity'] == commodity].copy()
    if df_c.empty:
        print("No rows for", commodity); continue

    # prepare series
    series = df_c.groupby('Date')['Average'].mean().sort_index()
    series = series.asfreq('D')
    series = series.interpolate(method='time').ffill().bfill()
    dates_full = series.index
    values_full = series.values

    # windows
    X_all, y_all = create_multistep_windows(values_full, INPUT_WINDOW, HORIZON)
    n_windows = len(X_all)
    n_test = int(n_windows * TEST_FRACTION)
    n_train = n_windows - n_test

    scaler = MinMaxScaler()
    scaler.fit(y_all[:n_train].reshape(-1,1))
    X_scaled = scaler.transform(X_all.reshape(-1,1)).reshape(X_all.shape)
    y_scaled = scaler.transform(y_all.reshape(-1,1)).reshape(y_all.shape)

    X_train = X_scaled[:n_train]; y_train = y_scaled[:n_train]
    X_test = X_scaled[n_train:]; y_test = y_scaled[n_train:]
    dates_test = dates_full[INPUT_WINDOW + n_train : INPUT_WINDOW + n_train + len(X_test)]

    X_train_c = X_train.reshape((-1, INPUT_WINDOW, 1))
    X_test_c = X_test.reshape((-1, INPUT_WINDOW, 1))

    results = {}

    # HPO done on just first produce type (for sake of training time)
    if not hpo_done:
        # split a small val set from training for HPO
        val_split = max(1, int(0.15 * len(X_train_c)))
        X_hpo_train, y_hpo_train = X_train_c[:-val_split], y_train[:-val_split]
        X_hpo_val, y_hpo_val = X_train_c[-val_split:], y_train[-val_split:]

        # define param spaces for each model
        cnn_space = {
            'filters1': [32,48,64],
            'filters2': [16,32],
            'dense': [32,64,128],
            'dropout': [0.1,0.2,0.3],
            'lr': [1e-3, 5e-4]
        }
        rnn_space = {
            'units': [32,40,64],
            'dense': [32,64],
            'dropout':[0.1,0.2],
            'lr':[1e-3,5e-4]
        }
        lstm_space = rnn_space.copy()
        gru_space = rnn_space.copy()
        tr_space = {
            'embed_dim':[32,64],
            'heads':[2,4],
            'ff_dim':[32,64],
            'blocks':[1,2],
            'dropout':[0.05,0.1],
            'lr':[1e-3,5e-4]
        }

        print("Starting HPO (random search) on first commodity...")
        # run random HPO for each model type
        shared_hyperparams['cnn'], _ = run_random_hpo('cnn', lambda p: build_cnn_from_params(p), cnn_space, X_hpo_train, y_hpo_train, X_hpo_val, y_hpo_val, trials=HPO_TRIALS)
        shared_hyperparams['rnn'], _ = run_random_hpo('rnn', lambda p: build_rnn_from_params(p), rnn_space, X_hpo_train, y_hpo_train, X_hpo_val, y_hpo_val, trials=HPO_TRIALS)
        shared_hyperparams['lstm'], _ = run_random_hpo('lstm', lambda p: build_lstm_from_params(p), lstm_space, X_hpo_train, y_hpo_train, X_hpo_val, y_hpo_val, trials=HPO_TRIALS)
        shared_hyperparams['gru'], _ = run_random_hpo('gru', lambda p: build_gru_from_params(p), gru_space, X_hpo_train, y_hpo_train, X_hpo_val, y_hpo_val, trials=HPO_TRIALS)
        shared_hyperparams['transformer'], _ = run_random_hpo('transformer', lambda p: build_transformer_from_params(p), tr_space, X_hpo_train, y_hpo_train, X_hpo_val, y_hpo_val, trials=HPO_TRIALS)

        hpo_done = True
        print("HPO complete, shared hyperparameters stored.")

    # Train models using shared_hyperparams (retrain on full training set)
    es = callbacks.EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)

    # CNN
    print("Training CNN")
    cnn_params = shared_hyperparams.get('cnn')
    cnn = build_cnn_from_params(cnn_params) if cnn_params is not None else build_cnn_from_params({'filters1':64,'filters2':32,'dense':64,'dropout':0.2,'lr':1e-3})
    history_cnn = cnn.fit(X_train_c, y_train, validation_split=0.15, epochs=BASE_EPOCHS, batch_size=BATCH_SIZE, callbacks=[es], verbose=1)
    pred_cnn_scaled = cnn.predict(X_test_c); pred_cnn = inv_scale_multi(pred_cnn_scaled, scaler)
    y_test_inv = inv_scale_multi(y_test, scaler)
    overall_rmse_cnn, overall_mae_cnn, per_rmse_cnn, per_mae_cnn = multistep_rmse_mae(y_test_inv, pred_cnn)
    results['cnn'] = {'pred': pred_cnn, 'y_test': y_test_inv, 'overall_rmse': overall_rmse_cnn, 'overall_mae': overall_mae_cnn, 'per_rmse': per_rmse_cnn, 'history': history_cnn}
    if SAVE_RESULTS: cnn.save(f"models/{commodity.replace(' ','_')}_cnn.keras")

    # Plot CNN results

    # plot training history
    plot_training_history(history_cnn, "CNN", commodity)
    # single model prediction (first horizon)
    n_pred = pred_cnn.shape[0]
    plot_single_model_prediction(dates_full, values_full, dates_test[:n_pred], pred_cnn[:,0], "CNN", commodity)
    # true vs pred
    plot_true_vs_pred_first_h(y_test_inv[:n_pred], pred_cnn, "CNN", commodity)
    plot_true_vs_pred_flatten(y_test_inv[:n_pred], pred_cnn, "CNN", commodity)

    # RNN
    print("Training RNN")
    rnn_params = shared_hyperparams.get('rnn')
    rnn = build_rnn_from_params(rnn_params) if rnn_params is not None else build_rnn_from_params({'units':40,'dense':64,'dropout':0.2,'lr':1e-3})
    history_rnn = rnn.fit(X_train_c, y_train, validation_split=0.15, epochs=BASE_EPOCHS, batch_size=max(1,int(BATCH_SIZE/2)), callbacks=[es], verbose=1)
    pred_rnn_scaled = rnn.predict(X_test_c); pred_rnn = inv_scale_multi(pred_rnn_scaled, scaler)
    overall_rmse_rnn, overall_mae_rnn, per_rmse_rnn, per_mae_rnn = multistep_rmse_mae(y_test_inv, pred_rnn)
    results['rnn'] = {'pred': pred_rnn, 'y_test': y_test_inv, 'overall_rmse': overall_rmse_rnn, 'overall_mae': overall_mae_rnn, 'per_rmse': per_rmse_rnn, 'history': history_rnn}
    if SAVE_RESULTS: rnn.save(f"models/{commodity.replace(' ','_')}_rnn.keras")

    # Plot RNN results
    plot_training_history(history_rnn, "RNN", commodity)
    n_pred = pred_rnn.shape[0]
    plot_single_model_prediction(dates_full, values_full, dates_test[:n_pred], pred_rnn[:,0], "RNN", commodity)
    plot_true_vs_pred_first_h(y_test_inv[:n_pred], pred_rnn, "RNN", commodity)
    plot_true_vs_pred_flatten(y_test_inv[:n_pred], pred_rnn, "RNN", commodity)

    # LSTM
    print("Training LSTM")
    lstm_params = shared_hyperparams.get('lstm')
    lstm = build_lstm_from_params(lstm_params) if lstm_params is not None else build_lstm_from_params({'units':64,'dense':64,'dropout':0.2,'lr':1e-3})
    history_lstm = lstm.fit(X_train_c, y_train, validation_split=0.15, epochs=BASE_EPOCHS, batch_size=max(1,int(BATCH_SIZE/2)), callbacks=[es], verbose=1)
    pred_lstm_scaled = lstm.predict(X_test_c); pred_lstm = inv_scale_multi(pred_lstm_scaled, scaler)
    overall_rmse_lstm, overall_mae_lstm, per_rmse_lstm, per_mae_lstm = multistep_rmse_mae(y_test_inv, pred_lstm)
    results['lstm'] = {'pred': pred_lstm, 'y_test': y_test_inv, 'overall_rmse': overall_rmse_lstm, 'overall_mae': overall_mae_lstm, 'per_rmse': per_rmse_lstm, 'history': history_lstm}
    if SAVE_RESULTS: lstm.save(f"models/{commodity.replace(' ','_')}_lstm.keras")

    # Plot LSTM results
    plot_training_history(history_lstm, "LSTM", commodity)
    n_pred = pred_lstm.shape[0]
    plot_single_model_prediction(dates_full, values_full, dates_test[:n_pred], pred_lstm[:,0], "LSTM", commodity)
    plot_true_vs_pred_first_h(y_test_inv[:n_pred], pred_lstm, "LSTM", commodity)
    plot_true_vs_pred_flatten(y_test_inv[:n_pred], pred_lstm, "LSTM", commodity)

    # GRU
    print("Training GRU")
    gru_params = shared_hyperparams.get('gru')
    gru = build_gru_from_params(gru_params) if gru_params is not None else build_gru_from_params({'units':64,'dense':64,'dropout':0.2,'lr':1e-3})
    history_gru = gru.fit(X_train_c, y_train, validation_split=0.15, epochs=BASE_EPOCHS, batch_size=max(1,int(BATCH_SIZE/2)), callbacks=[es], verbose=1)
    pred_gru_scaled = gru.predict(X_test_c); pred_gru = inv_scale_multi(pred_gru_scaled, scaler)
    overall_rmse_gru, overall_mae_gru, per_rmse_gru, per_mae_gru = multistep_rmse_mae(y_test_inv, pred_gru)
    results['gru'] = {'pred': pred_gru, 'y_test': y_test_inv, 'overall_rmse': overall_rmse_gru, 'overall_mae': overall_mae_gru, 'per_rmse': per_rmse_gru, 'history': history_gru}
    if SAVE_RESULTS: gru.save(f"models/{commodity.replace(' ','_')}_gru.keras")

    # Plot GRU results
    plot_training_history(history_gru, "GRU", commodity)
    n_pred = pred_gru.shape[0]
    plot_single_model_prediction(dates_full, values_full, dates_test[:n_pred], pred_gru[:,0], "GRU", commodity)
    plot_true_vs_pred_first_h(y_test_inv[:n_pred], pred_gru, "GRU", commodity)
    plot_true_vs_pred_flatten(y_test_inv[:n_pred], pred_gru, "GRU", commodity)

    # Transformer
    print("Training Transformer")
    tr_params = shared_hyperparams.get('transformer')
    transformer = build_transformer_from_params(tr_params) if tr_params is not None else build_transformer_from_params({'embed_dim':64,'heads':4,'ff_dim':64,'blocks':2,'dropout':0.1,'lr':1e-3})
    history_tr = transformer.fit(X_train_c, y_train, validation_split=0.15, epochs=BASE_EPOCHS, batch_size=BATCH_SIZE, callbacks=[es], verbose=1)
    pred_tr_scaled = transformer.predict(X_test_c); pred_tr = inv_scale_multi(pred_tr_scaled, scaler)
    overall_rmse_tr, overall_mae_tr, per_rmse_tr, per_mae_tr = multistep_rmse_mae(y_test_inv, pred_tr)
    results['transformer'] = {'pred': pred_tr, 'y_test': y_test_inv, 'overall_rmse': overall_rmse_tr, 'overall_mae': overall_mae_tr, 'per_rmse': per_rmse_tr, 'history': history_tr}
    if SAVE_RESULTS: transformer.save(f"models/{commodity.replace(' ','_')}_transformer.keras")

    # Plot Transformer results
    plot_training_history(history_tr, "Transformer", commodity)
    n_pred = pred_tr.shape[0]
    plot_single_model_prediction(dates_full, values_full, dates_test[:n_pred], pred_tr[:,0], "Transformer", commodity)
    plot_true_vs_pred_first_h(y_test_inv[:n_pred], pred_tr, "Transformer", commodity)
    plot_true_vs_pred_flatten(y_test_inv[:n_pred], pred_tr, "Transformer", commodity)

    # ARIMA
    print("Running ARIMA")
    arima_res = run_arima_forecast(series.values, train_ratio=1 - TEST_FRACTION, order=(1,1,1), horizon=HORIZON)
    results['arima'] = arima_res
    # Plot ARIMA results (no training history)
    if arima_res['Y_pred'] is not None:
        n_pred = arima_res['Y_pred'].shape[0]
        plot_single_model_prediction(dates_full, values_full, dates_test[:n_pred], arima_res['Y_pred'][:,0], "ARIMA", commodity)
        plot_true_vs_pred_first_h(arima_res['Y_true'], arima_res['Y_pred'], "ARIMA", commodity)
        plot_true_vs_pred_flatten(arima_res['Y_true'], arima_res['Y_pred'], "ARIMA", commodity)

    # SARIMA
    print("Running SARIMA")
    sarima_res = run_iterative_sarima(series.values, train_ratio=1 - TEST_FRACTION, order=(1,1,1), seasonal_order=(0,1,1,7), refit_every=10)
    results['sarima'] = sarima_res
    # Plot SARIMA results
    if sarima_res['Y_pred'] is not None:
        n_pred = sarima_res['Y_pred'].shape[0]
        plot_single_model_prediction(dates_full, values_full, dates_test[:n_pred], sarima_res['Y_pred'][:,0], "SARIMA", commodity)
        plot_true_vs_pred_first_h(sarima_res['Y_true'], sarima_res['Y_pred'], "SARIMA", commodity)
        plot_true_vs_pred_flatten(sarima_res['Y_true'], sarima_res['Y_pred'], "SARIMA", commodity)

    # Combined per-horizon RMSE plot for each model
    preds_for_plot = {
        'CNN': results['cnn']['pred'],
        'RNN': results['rnn']['pred'],
        'LSTM': results['lstm']['pred'],
        'GRU': results['gru']['pred'],
        'Transformer': results['transformer']['pred']
    }
    if results['arima']['Y_pred'] is not None:
        preds_for_plot['ARIMA'] = results['arima']['Y_pred']
    if results['sarima']['Y_pred'] is not None:
        preds_for_plot['SARIMA'] = results['sarima']['Y_pred']

    eff_sizes = [v.shape[0] for v in preds_for_plot.values()]
    eff_min = min(eff_sizes) if len(eff_sizes)>0 else 0
    trimmed_preds = {k: v[:eff_min] for k,v in preds_for_plot.items()}
    y_test_true = results['cnn']['y_test'][:eff_min]

    # Keep the combined per-horizon RMSE plot
    plt.figure(figsize=(10,4))
    for name,pred in trimmed_preds.items():
        rmse_h = np.sqrt(np.mean((y_test_true - pred)**2, axis=0))
        plt.plot(range(1, HORIZON+1), rmse_h, marker='o', label=name)
    plt.title(f'{commodity} â€” Per-horizon RMSE (1..{HORIZON})'); plt.xlabel('Horizon (days)'); plt.ylabel('RMSE')
    plt.legend(); plt.grid(True); plt.show()

    # Summarize
    summary = {'commodity': commodity}
    for name in ['cnn','rnn','lstm','gru','transformer']:
        summary[f'{name}_rmse'] = results[name]['overall_rmse']
        summary[f'{name}_mae'] = results[name]['overall_mae']
    summary['arima_rmse'] = results['arima']['overall_rmse']; summary['arima_mae'] = results['arima']['overall_mae']
    summary['sarima_rmse'] = results['sarima']['overall_rmse']; summary['sarima_mae'] = results['sarima']['overall_mae']
    final_table.append(summary)

    # Save scaler & results optionally
    if SAVE_RESULTS:
        joblib.dump(scaler, f"models/{commodity.replace(' ','_')}_scaler.pkl")
        joblib.dump(results, f"models/{commodity.replace(' ','_')}_results.pkl")
        print("Saved models and scaler for", commodity)
    else:
        print("SAVE_RESULTS is False -> not saving models/scalers/results for", commodity)

# Final output and optional result saving
cmp_df = pd.DataFrame(final_table).set_index('commodity')
print("\nFINAL RMSE & MAE SUMMARY (multistep 10-day):")
print(cmp_df)
print("\nPipeline complete.")

if SAVE_RESULTS:
    cmp_df.to_csv('models/final_model_comparison.csv')
    print("Saved final table -> models/final_model_comparison.csv")
else:
    print("SAVE_RESULTS is False -> final table not saved to disk.")

